{
  "model_name": "meta-llama/Meta-Llama-3-8B",
  "train_data_path": "./data/processed_final/train.jsonl",
  "val_data_path": "./data/processed_final/val.jsonl",
  "output_dir": "./checkpoints",
  "num_epochs": 3,
  "per_device_train_batch_size": 4,
  "per_device_eval_batch_size": 8,
  "gradient_accumulation_steps": 4,
  "learning_rate": 2e-5,
  "warmup_steps": 100,
  "max_steps": -1,
  "max_grad_norm": 1.0,
  "max_seq_length": 8192,
  "eval_strategy": "steps",
  "eval_steps": 500,
  "save_strategy": "steps",
  "save_steps": 500,
  "save_total_limit": 3,
  "load_best_model_at_end": true,
  "metric_for_best_model": "eval_loss",
  "fp16": true,
  "dataloader_num_workers": 4,
  "logging_steps": 50,
  "report_to": "wandb",
  "seed": 42,
  "wandb_project": "amour-training",
  "wandb_run_name": null
}

