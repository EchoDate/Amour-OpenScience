### model
model_name_or_path: meta-llama/Meta-Llama-3.1-8B
trust_remote_code: true
flash_attn: fa2
### method
stage: sft
do_train: true
finetuning_type: full
# ### lora ( LoRA，)
# lora_target: all
# lora_rank: 16
# lora_alpha: 32
# lora_dropout: 0.05
### dataset
dataset: amour_dataset  # 
dataset_dir: ./data/llamafactory  # 
template: llama3
cutoff_len: 8192
overwrite_cache: true
preprocessing_num_workers: 8  # worker
packing: true
### output
output_dir: ./checkpoints/llama3_1_8b_full_8k_sft
logging_steps: 5
save_steps: 200
plot_loss: true
overwrite_output_dir: true
### train
per_device_train_batch_size: 4  # H80batch size
gradient_accumulation_steps: 8  # batch size
gradient_checkpointing: true
learning_rate: 1.0e-5
num_train_epochs: 3
lr_scheduler_type: cosine
warmup_ratio: 0.05
optim: adamw_torch
weight_decay: 0.01
max_grad_norm: 1.0
bf16: true                      
fp16: false
ddp_timeout: 180000000
### eval
val_size: 0.1  #  10% （，）
per_device_eval_batch_size: 4
eval_strategy: steps
eval_steps: 200
do_eval: true  # 
### deepspeed
deepspeed: deepspeed_config_2xh80_full.json
### wandb
report_to: wandb